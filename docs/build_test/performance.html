<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Performance · Trixi.jl</title><script data-outdated-warner src="assets/warner.js"></script><link rel="canonical" href="https://trixi-framework.github.io/Trixi.jl/stable/performance.html"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script><link href="assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="index.html">Trixi.jl</a></span></div><form class="docs-search" action="search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="index.html">Home</a></li><li><span class="tocitem">Getting started</span><ul><li><a class="tocitem" href="overview.html">Overview</a></li><li><a class="tocitem" href="visualization.html">Visualization</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="tutorials/introduction.html">Introduction</a></li><li><a class="tocitem" href="tutorials/getting_started_with_Trixi.html">1 Getting started with Trixi</a></li><li><a class="tocitem" href="tutorials/scalar_linear_advection_1d.html">2 Introduction to DG methods</a></li><li><a class="tocitem" href="tutorials/DGSEM_FluxDiff.html">3 DGSEM with flux differencing</a></li><li><a class="tocitem" href="tutorials/shock_capturing.html">4 Shock capturing with flux differencing and stage limiter</a></li><li><a class="tocitem" href="tutorials/non_periodic_boundaries.html">5 Non-periodic boundaries</a></li><li><a class="tocitem" href="tutorials/DGMulti_1.html">6 DG schemes via <code>DGMulti</code> solver</a></li><li><a class="tocitem" href="tutorials/DGMulti_2.html">7 Other SBP schemes (FD, CGSEM) via <code>DGMulti</code> solver</a></li><li><a class="tocitem" href="tutorials/upwind_fdsbp.html">8 Upwind FD SBP schemes</a></li><li><a class="tocitem" href="tutorials/adding_new_scalar_equations.html">9 Adding a new scalar conservation law</a></li><li><a class="tocitem" href="tutorials/adding_nonconservative_equation.html">10 Adding a non-conservative equation</a></li><li><a class="tocitem" href="tutorials/parabolic_terms.html">11 Parabolic terms</a></li><li><a class="tocitem" href="tutorials/adding_new_parabolic_terms.html">12 Adding new parabolic terms</a></li><li><a class="tocitem" href="tutorials/adaptive_mesh_refinement.html">13 Adaptive mesh refinement</a></li><li><a class="tocitem" href="tutorials/structured_mesh_mapping.html">14 Structured mesh with curvilinear mapping</a></li><li><a class="tocitem" href="tutorials/hohqmesh_tutorial.html">15 Unstructured meshes with HOHQMesh.jl</a></li><li><a class="tocitem" href="tutorials/time_stepping.html">16 Explicit time stepping</a></li><li><a class="tocitem" href="tutorials/differentiable_programming.html">17 Differentiable programming</a></li></ul></li><li><span class="tocitem">Basic building blocks</span><ul><li><input class="collapse-toggle" id="menuitem-4-1" type="checkbox"/><label class="tocitem" for="menuitem-4-1"><span class="docs-label">Meshes</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="meshes/tree_mesh.html">Tree mesh</a></li><li><a class="tocitem" href="meshes/structured_mesh.html">Structured mesh</a></li><li><a class="tocitem" href="meshes/unstructured_quad_mesh.html">Unstructured mesh</a></li><li><a class="tocitem" href="meshes/p4est_mesh.html">P4est-based mesh</a></li><li><a class="tocitem" href="meshes/dgmulti_mesh.html">DGMulti mesh</a></li></ul></li><li><a class="tocitem" href="time_integration.html">Time integration</a></li><li><a class="tocitem" href="callbacks.html">Callbacks</a></li></ul></li><li><span class="tocitem">Advanced topics &amp; developers</span><ul><li><a class="tocitem" href="conventions.html">Conventions</a></li><li><a class="tocitem" href="development.html">Development</a></li><li><a class="tocitem" href="github-git.html">GitHub &amp; Git</a></li><li><a class="tocitem" href="styleguide.html">Style guide</a></li><li><a class="tocitem" href="testing.html">Testing</a></li><li class="is-active"><a class="tocitem" href="performance.html">Performance</a><ul class="internal"><li><a class="tocitem" href="#Manual-benchmarking"><span>Manual benchmarking</span></a></li></ul></li><li><a class="tocitem" href="parallelization.html">Parallelization</a></li></ul></li><li><a class="tocitem" href="troubleshooting.html">Troubleshooting and FAQ</a></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="reference-trixi.html">Trixi.jl</a></li><li><a class="tocitem" href="reference-trixi2vtk.html">Trixi2Vtk.jl</a></li></ul></li><li><a class="tocitem" href="authors.html">Authors</a></li><li><a class="tocitem" href="contributing.html">Contributing</a></li><li><a class="tocitem" href="code_of_conduct.html">Code of Conduct</a></li><li><a class="tocitem" href="license.html">License</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Advanced topics &amp; developers</a></li><li class="is-active"><a href="performance.html">Performance</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="performance.html">Performance</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com//blob/master/docs/src/performance.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Performance"><a class="docs-heading-anchor" href="#Performance">Performance</a><a id="Performance-1"></a><a class="docs-heading-anchor-permalink" href="#Performance" title="Permalink"></a></h1><p>Trixi.jl is designed to balance performance and readability. Since Julia provides a lot of zero-cost abstractions, it is often possible to optimize both goals simultaneously.</p><p>The usual development workflow in Julia is</p><ol><li>Make it work.</li><li>Make it nice.</li><li>Make it fast.</li></ol><p>To achieve the third step, you should be familiar with (at least) the section on <a href="https://docs.julialang.org/en/v1/manual/performance-tips/">performance tips in the Julia manual</a>. Here, we just list some important aspects you should consider when developing Trixi.</p><ul><li>Consider using <code>@views</code>/<code>view(...)</code> when using array slices, except on the left-side of an assignment (<a href="https://docs.julialang.org/en/v1/manual/performance-tips/#man-performance-views">further details</a>).</li><li>Functions are essentially for free, since they are usually automatically inlined where it makes sense (using <code>@inline</code> can be used as an additional hint to the compiler) (<a href="https://docs.julialang.org/en/v1/manual/performance-tips/#Break-functions-into-multiple-definitions">further details</a>).</li><li>Function barriers can improve performance due to type stability (<a href="https://docs.julialang.org/en/v1/manual/performance-tips/#kernel-functions">further details</a>).</li><li>Look for type instabilities using <code>@code_warntype</code>. Consider using <code>@descend</code> from <a href="https://github.com/JuliaDebug/Cthulhu.jl">Cthulhu.jl</a> to investigate deeper call chains.</li></ul><h2 id="Manual-benchmarking"><a class="docs-heading-anchor" href="#Manual-benchmarking">Manual benchmarking</a><a id="Manual-benchmarking-1"></a><a class="docs-heading-anchor-permalink" href="#Manual-benchmarking" title="Permalink"></a></h2><p>If you modify some internal parts of Trixi, you should check the impact on performance. Hence, you should at least investigate the performance roughly by comparing the reported timings of several elixirs. Deeper investigations and micro-benchmarks should usually use <a href="https://github.com/JuliaCI/BenchmarkTools.jl">BenchmarkTools.jl</a>. For example, the following steps were used to benchmark the changes introduced in https://github.com/trixi-framework/Trixi.jl/pull/256.</p><ol><li><code>git checkout e7ebf3846b3fd62ee1d0042e130afb50d7fe8e48</code> (new version)</li><li>Start <code>julia --threads=1 --check-bounds=no</code>.</li><li>Execute the following code in the REPL to benchmark the <code>rhs!</code> call at the final state. ```julia julia&gt; using BenchmarkTools, Revise; using Trixi</li></ol><p>julia&gt; trixi<em>include(&quot;examples/2d/elixir</em>euler<em>sedov</em>blast_wave.jl&quot;)</p><p>julia&gt; du<em>test = copy(sol.u[end]); u</em>test = copy(sol.u[end]);</p><p>julia&gt; @benchmark Trixi.rhs!(              du_test,              u_test,              semi,              sol.t[end])    BenchmarkTools.Trial:     memory estimate:  10.48 KiB     allocs estimate:  67     –––––––     minimum time:     4.510 ms (0.00% GC)     median time:      4.646 ms (0.00% GC)     mean time:        4.699 ms (0.00% GC)     maximum time:     7.183 ms (0.00% GC)     –––––––     samples:          1065     evals/sample:     1</p><p>shell&gt; git checkout 222241ff54f8a4ca9876cc1fc25ae262416a4ea0</p><p>julia&gt; trixi<em>include(&quot;examples/2d/elixir</em>euler<em>sedov</em>blast_wave.jl&quot;)</p><p>julia&gt; @benchmark Trixi.rhs!(              du_test,              u_test,              semi,              sol.t[end])    BenchmarkTools.Trial:     memory estimate:  10.36 KiB     allocs estimate:  67     –––––––     minimum time:     4.500 ms (0.00% GC)     median time:      4.635 ms (0.00% GC)     mean time:        4.676 ms (0.00% GC)     maximum time:     5.880 ms (0.00% GC)     –––––––     samples:          1070     evals/sample:     1</p><pre><code class="nohighlight hljs">   Run the `@benchmark ...` commands multiple times to see whether there are any significant fluctuations.

Follow these steps for both commits you want to compare. The relevant benchmark results you should typically be looking at
are the median and mean values of the runtime and the memory/allocs estimate. In this example, the differences
of the runtimes are of the order of the fluctuations one gets when running the benchmarks multiple times. Since
the memory/allocs are (roughly) the same, there doesn&#39;t seem to be a significant performance regression here.

You can also make it more detailed by benchmarking only, e.g., the calculation of the volume terms, but whether that&#39;s necessary depends on the modifications you made and their (potential) impact.

Some more detailed description of manual profiling and benchmarking as well as
resulting performance improvements of Trixi are given in the following blog posts.
- [Improving performance of AMR with p4est](https://ranocha.de/blog/Optimizing_p4est_AMR/),
  cf. [#638](https://github.com/trixi-framework/Trixi.jl/pull/638)
- [Improving performance of EC methods](https://ranocha.de/blog/Optimizing_EC_Trixi/),
  cf. [#643](https://github.com/trixi-framework/Trixi.jl/pull/643)


## Automated benchmarking

We use [PkgBenchmark.jl](https://github.com/JuliaCI/PkgBenchmark.jl) to provide a standard set of
benchmarks for Trixi. The relevant benchmark script is
[benchmark/benchmarks.jl](https://github.com/trixi-framework/Trixi.jl/blob/main/benchmark/benchmarks.jl).
You can run a standard set of benchmarks via</code></pre><p>julia julia&gt; using PkgBenchmark, Trixi</p><p>julia&gt; results = benchmarkpkg(Trixi, BenchmarkConfig(juliacmd=<code>$(Base.julia_cmd()) --check-bounds=no --threads=1</code>))</p><p>julia&gt; export<em>markdown(joinpath(pathof(Trixi) |&gt; dirname |&gt; dirname, &quot;benchmark&quot;, &quot;single</em>benchmark.md&quot;), results)</p><pre><code class="nohighlight hljs">This will save a markdown file with a summary of the benchmark results similar to
[this example](https://gist.github.com/ranocha/494fa2529e1e6703c17b08434c090980).
Note that this will take quite some time. Additional options are described in the
[docs of PkgBenchmark.jl](https://juliaci.github.io/PkgBenchmark.jl/stable).
A particularly useful option is to specify a `BenchmarkConfig` including Julia
command line options affecting the performance such as disabling bounds-checking
and setting the number of threads.

A useful feature when developing Trixi is to compare the performance of Trixi&#39;s
current state vs. the `main` branch. This can be achieved by executing</code></pre><p>julia julia&gt; using PkgBenchmark, Trixi</p><p>julia&gt; results = judge(Trixi,              BenchmarkConfig(juliacmd=<code>$(Base.julia_cmd()) --check-bounds=no --threads=1</code>), # target              BenchmarkConfig(juliacmd=<code>$(Base.julia_cmd()) --check-bounds=no --threads=1</code>, id=&quot;main&quot;) # baseline        )</p><p>julia&gt; export_markdown(joinpath(pathof(Trixi) |&gt; dirname |&gt; dirname, &quot;benchmark&quot;, &quot;results.md&quot;), results)</p><pre><code class="nohighlight hljs">By default, the `target` is the current state of the repository. Remember that you
need to be in a clean state (commit or stash your changes) to run this successfully.
You can also run this comparison and an additional one using two threads via</code></pre><p>julia julia&gt; include(&quot;benchmark/run_benchmarks.jl&quot;)</p><pre><code class="nohighlight hljs">Then, markdown files including the results are saved in `benchmark/`.
[This example result](https://gist.github.com/ranocha/bf98d19e288e759d3a36ca0643448efb)
was obtained using a GitHub action for the
[PR #535](https://github.com/trixi-framework/Trixi.jl/pull/535).
Note that GitHub actions run on in the cloud in a virtual machine. Hence, we do not really
have control over it and performance results must be taken with a grain of salt.
Nevertheless, significant runtime differences and differences of memory allocations
should be robust indicators of performance changes.


## Runtime performance vs. latency aka using `@nospecialize` selectively

Usually, Julia will compile specialized versions of each method, using as much information from the
types of function arguments as possible (based on some
[heuristics](https://docs.julialang.org/en/v1/manual/performance-tips/#Be-aware-of-when-Julia-avoids-specializing)).
The compiler will generate code that is as efficient as comparable code written in a low-level
language such as C or Fortran. However, there are cases where the runtime performance does not
really matter but the time needed to compile specializations becomes significant. This is related to
latency or the time-to-first-plot problem, well-known in the Julia community. In such a case, it can
be useful to remove some burden from the compiler by avoiding specialization on every possible argument
types using [the macro `@nospecialize`](https://docs.julialang.org/en/v1/base/base/#Base.@nospecialize).
A prime example of such a case is pretty printing of `struct`s in the Julia REPL, see the
[associated PR](https://github.com/trixi-framework/Trixi.jl/pull/447) for further discussions.

As a rule of thumb:
- Do not use `@nospecialize` in performance-critical parts, in particular not for methods involved
  in computing `Trixi.rhs!`.
- Consider using `@nospecialize` for methods like custom implementations of `Base.show`.


## Performance metrics of the `AnalysisCallback`
The [`AnalysisCallback`](@ref) computes two performance indicators that you can use to
evaluate the serial and parallel performance of Trixi. They represent
measured run times that are normalized by the number of `rhs!` evaluations and
the number of degrees of freedom of the problem setup. The normalization ensures that we can
compare different measurements for each type of indicator independent of the number of
time steps or mesh size. All indicators have in common that they are still in units of
time, thus *lower is better* for each of them.

Here, the term &quot;degrees of freedom&quot; (DOFs) refers to the number of *independent*
state vectors that are used to represent the numerical solution. For example, if
you use a DGSEM-type scheme in 2D on a mesh with 8 elements and with
5-by-5 Gauss-Lobatto nodes in each element (i.e., a polynomial degree of 4), the
total number of DOFs would be</code></pre><p>math n_\text{DOFs,DGSEM} = {\text{number of elements}} \cdot {\text{number of nodes per element}} = 8 \cdot (5 \cdot 5) = 200.</p><pre><code class="nohighlight hljs">In contrast, for a finite volume-type scheme on a mesh with 8 elements, the total number of
DOFs would be (independent of the number of spatial dimensions)</code></pre><p>math n_\text{DOFs,FV} = {\text{number of elements}} = 8,</p><pre><code class="nohighlight hljs">since for standard finite volume methods you store a single state vector in each
element. Note that we specifically count the number of state *vectors* and not
the number of state *variables* for the DOFs. That is, in the previous example
``n_\text{DOFs,FV}`` is equal to 8 independent of whether this is a compressible Euler
setup with 5 state variables or a linear scalar advection setup with one state
variable.

For each indicator, the measurements are always since the last invocation of the
`AnalysisCallback`. That is, if the analysis callback is called multiple times,
the indicators are repeatedly computed and can thus also be used to track the
performance over the course of a longer simulation, e.g., to analyze setups with varying performance
characteristics. Note that the time spent in the `AnalysisCallback` itself is always
*excluded*, i.e., the performance measurements are not distorted by potentially
expensive solution analysis computations. All other parts of a Trixi simulation
are included, however, thus make sure that you disable everything you do *not*
want to be measured (such as I/O callbacks, visualization etc.).

!!! note &quot;Performance indicators and adaptive mesh refinement&quot;
    Currently it is not possible to compute meaningful performance indicators for a simulation
    with arbitrary adaptive mesh refinement, since this would require to
    explicitly keep track of the number of DOF updates due to the mesh size
    changing repeatedly. The only way to do this at the moment is by setting the
    analysis interval to the same value as the AMR interval.

### Local, `rhs!`-only indicator
The *local, `rhs!`-only indicator* is computed as</code></pre><p>math \text{time/DOF/rhs!} = \frac{t<em>\text{\texttt{rhs!}}}{n</em>\text{DOFs,local} \cdot n_\text{calls,\texttt{rhs!}}},</p><pre><code class="nohighlight hljs">where ``t_\text{\texttt{rhs!}}`` is the accumulated time spent in `rhs!`,
``n_\text{DOFs,local}`` is the *local* number of DOFs (i.e., on the
current MPI rank; if doing a serial run, you can just think of this as *the*
number of DOFs), and ``n_\text{calls,\texttt{rhs!}}`` is the number of times the
`rhs!` function has been evaluated. Note that for this indicator, we measure *only*
the time spent in `rhs!`, i.e., by definition all computations outside of `rhs!` - specifically
all other callbacks and the time integration method - are not taken into account.

The local, `rhs!`-only indicator is usually most useful if you do serial
measurements and are interested in the performance of the implementation of your
core numerical methods (e.g., when doing performance tuning).

### Performance index (PID)
The *performance index* (PID) is computed as</code></pre><p>math \text{PID} = \frac{t<em>\text{wall} \cdot n</em>\text{ranks,MPI}}{n<em>\text{DOFs,global} \cdot n</em>\text{calls,\texttt{rhs!}}}, `<span>$where$</span>t<em>\text{wall}<span>$is the walltime since the last call to the `AnalysisCallback`,$</span>n</em>\text{ranks,MPI}<span>$is the number of MPI ranks used,$</span>n<em>\text{DOFs,global}<span>$is the *global* number of DOFs (i.e., the sum of DOFs over all MPI ranks; if doing a serial run, you can just think of this as *the* number of DOFs), and$</span>n</em>\text{calls,\texttt{rhs!}}`<code>is the number of times the</code>rhs!<code>function has been evaluated since the last call to the</code>AnalysisCallback<code>. The PID measures everything except the time spent in the</code>AnalysisCallback` itself - specifically, all other callbacks and the time integration method itself are included.</p><p>The PID is usually most useful if you would like to compare the parallel performance of your code to its serial performance. Specifically, it allows you to evaluate the parallelization overhead of your code by giving you a measure of the resources that are necessary to solve a given simulation setup. In a sense, it mimics the &quot;core hours&quot; metric that is often used by supercomputer centers to measure how many resources a particular compute job requires. It can thus be seen as a proxy for &quot;energy used&quot; and, as an extension, &quot;monetary cost&quot;.</p><div class="admonition is-info"><header class="admonition-header">Initialization overhead in measurements</header><div class="admonition-body"><p>When using one of the integration schemes from OrdinaryDiffEq.jl, their implementation will initialize some OrdinaryDiffEq.jl-specific information during the first time step. Among other things, one additional call to <code>rhs!</code> is performed. Therefore, make sure that for performance measurements using the PID either the number of timesteps or the workload per <code>rhs!</code> call is large enough to make the initialization overhead negligible. Note that the extra call to <code>rhs!</code> is properly accounted for in both the number of calls and the measured time, so you do not need to worry about it being expensive. If you want a perfect timing result, you need to set the analysis interval such that the <code>AnalysisCallback</code> is invoked at least once during the course of the simulation and discard the first PID value.</p></div></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="testing.html">« Testing</a><a class="docs-footer-nextpage" href="parallelization.html">Parallelization »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Monday 17 April 2023 08:36">Monday 17 April 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
